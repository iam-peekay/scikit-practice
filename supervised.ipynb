{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "iris_X = iris.data\n",
    "iris_y = iris.target\n",
    "\n",
    "np.unique(iris_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  [1 2 1 0 0 0 2 1 2 0]\n",
      "Actual:      [1 1 1 0 0 0 2 1 2 0]\n"
     ]
    }
   ],
   "source": [
    "# Split iris data in train and test data\n",
    "# A random permutation, to split the data randomly\n",
    "np.random.seed(0)\n",
    "\n",
    "indices = np.random.permutation(len(iris_X))\n",
    "iris_X_train = iris_X[indices[:-10]]\n",
    "iris_y_train = iris_y[indices[:-10]]\n",
    "iris_X_test  = iris_X[indices[-10:]]\n",
    "iris_y_test  = iris_y[indices[-10:]]\n",
    "\n",
    "# Create and fit a nearest-neighbor classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(iris_X_train, iris_y_train)\n",
    "\n",
    "# Predict based on model fit\n",
    "p = knn.predict(iris_X_test)\n",
    "\n",
    "print('Prediction: ', p)\n",
    "print('Actual:     ', iris_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  3.03499549e-01  -2.37639315e+02   5.10530605e+02   3.27736980e+02\n",
      "  -8.14131709e+02   4.92814588e+02   1.02848452e+02   1.84606489e+02\n",
      "   7.43519617e+02   7.60951722e+01]\n",
      "Mean square error:  2004.56760269\n",
      "Variance score:  0.585075302269\n"
     ]
    }
   ],
   "source": [
    "# Linear model\n",
    "\n",
    "# Linear regression\n",
    "diabetes = datasets.load_diabetes()\n",
    "diabetes_X_train = diabetes.data[:-20]\n",
    "diabetes_X_test  = diabetes.data[-20:]\n",
    "diabetes_y_train = diabetes.target[:-20]\n",
    "diabetes_y_test  = diabetes.target[-20:]\n",
    "\n",
    "from sklearn import linear_model\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "regr.fit(diabetes_X_train, diabetes_y_train)\n",
    "print(regr.coef_)\n",
    "\n",
    "# m=Mean square error\n",
    "m = np.mean((regr.predict(diabetes_X_test)-diabetes_y_test)**2)\n",
    "print('Mean square error: ', m)\n",
    "\n",
    "# Variance score: 1 is perfect prediction and 0 means \n",
    "# that there is no linear relationship between X and y.\n",
    "vs = regr.score(diabetes_X_test, diabetes_y_test) \n",
    "print('Variance score: ', vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shrinkage\n",
    "\n",
    "# If there are few data points per dimension, \n",
    "# noise in the observations induces high variance:\n",
    "X = np.c_[ .5, 1].T\n",
    "y = [.5, 1]\n",
    "test = np.c_[ 0, 2].T\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "plt.figure() \n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "for _ in range(6): \n",
    "    this_X = .1*np.random.normal(size=(2, 1)) + X\n",
    "    \n",
    "    regr.fit(this_X, y)\n",
    "    plt.plot(test, regr.predict(test))\n",
    "    plt.scatter(this_X, y, s=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A solution in high-dimensional statistical learning is to shrink\n",
    "# the regression coefficients to zero: any two randomly chosen set of \n",
    "# observations are likely to be uncorrelated. This is called Ridge regression:\n",
    "regr = linear_model.Ridge(alpha=.1)\n",
    "\n",
    "plt.figure() \n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "for _ in range(6): \n",
    "    this_X = .1*np.random.normal(size=(2, 1)) + X\n",
    "    regr.fit(this_X, y)\n",
    "    plt.plot(test, regr.predict(test)) \n",
    "    plt.scatter(this_X, y, s=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.58511106838835303, 0.58520730154446765, 0.58546775406984897, 0.58555120365039159, 0.58307170855541623, 0.57058999437280089]\n"
     ]
    }
   ],
   "source": [
    "# This is an example of bias/variance tradeoff: \n",
    "# the larger the ridge alpha parameter, the higher the bias and the lower the variance.\n",
    "# We can choose alpha to minimize left out error, \n",
    "# this time using the diabetes dataset rather than our synthetic data:\n",
    "\n",
    "alphas = np.logspace(-4, -1, 6)\n",
    "from __future__ import print_function\n",
    "print([regr.set_params(alpha=alpha\n",
    "                    ).fit(diabetes_X_train, diabetes_y_train,\n",
    "                    ).score(diabetes_X_test, diabetes_y_test) for alpha in alphas])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0.         -212.43764548  517.19478111  313.77959962 -160.8303982    -0.\n",
      " -187.19554705   69.38229038  508.66011217   71.84239008]\n"
     ]
    }
   ],
   "source": [
    "# Sparsity\n",
    "regr = linear_model.Lasso()\n",
    "scores = [regr.set_params(alpha=alpha\n",
    "             ).fit(diabetes_X_train, diabetes_y_train\n",
    "             ).score(diabetes_X_test, diabetes_y_test)\n",
    "        for alpha in alphas]\n",
    "\n",
    "best_alpha = alphas[scores.index(max(scores))]\n",
    "regr.alpha = best_alpha\n",
    "\n",
    "regr.fit(diabetes_X_train, diabetes_y_train)\n",
    "\n",
    "print(regr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classification\n",
    "\n",
    "logistic = linear_model.LogisticRegression(C=1e5)\n",
    "logistic.fit(iris_X_train, iris_y_train)\n",
    "\n",
    "# The C parameter controls the amount of regularization \n",
    "# in the LogisticRegression object: \n",
    "# a large value for C results in less regularization. \n",
    "# penalty=\"l2\" gives Shrinkage (i.e. non-sparse coefficients), while penalty=\"l1\" gives Sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SVMs\n",
    "from sklearn import svm\n",
    "\n",
    "svc = svm.SVC(kernel='linear')\n",
    "svc.fit(iris_X_train, iris_y_train)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
